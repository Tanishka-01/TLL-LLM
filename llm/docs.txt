
# setup ollama to get llm
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

# after install check if working
```bash
ollama list
```

# install model or run script to install one from hugging face
```bash
ollama pull deepseek-r1:1.5b
```

# then test it by running it
```bash
ollama run deepseek-r1:1.5b
```

#TODO
source sep to get context from person then pass to tokenizer
every 20s get transcript: id, timestamp, and text from id


the deepseek model is too slow and adds to much talk, so we need to make a modelfile to make the conv shorter and give context to the data coming in.
